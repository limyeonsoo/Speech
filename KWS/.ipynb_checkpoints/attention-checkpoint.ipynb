{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"Train a CNN for Google speech commands.\"\"\"\n",
    "\n",
    "__author__ = 'Yuan Xu, Erdene-Ochir Tuguldur'\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "\n",
    "from tqdm import *\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "\n",
    "import torchvision\n",
    "from torchvision.transforms import *\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import models\n",
    "from datasets import *\n",
    "from transforms import *\n",
    "from mixup import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseRNN(nn.Module):\n",
    "    r\"\"\"\n",
    "    Applies a multi-layer RNN to an input sequence.\n",
    "    Note:\n",
    "        Do not use this class directly, use one of the sub classes.\n",
    "    Args:\n",
    "        vocab_size (int): size of the vocabulary\n",
    "        max_len (int): maximum allowed length for the sequence to be processed\n",
    "        hidden_size (int): number of features in the hidden state `h`\n",
    "        input_dropout_p (float): dropout probability for the input sequence\n",
    "        dropout_p (float): dropout probability for the output sequence\n",
    "        n_layers (int): number of recurrent layers\n",
    "        rnn_cell (str): type of RNN cell (Eg. 'LSTM' , 'GRU')\n",
    "\n",
    "    Inputs: ``*args``, ``**kwargs``\n",
    "        - ``*args``: variable length argument list.\n",
    "        - ``**kwargs``: arbitrary keyword arguments.\n",
    "\n",
    "    Attributes:\n",
    "        SYM_MASK: masking symbol\n",
    "        SYM_EOS: end-of-sequence symbol\n",
    "    \"\"\"\n",
    "    SYM_MASK = \"MASK\"\n",
    "    SYM_EOS = \"EOS\"\n",
    "\n",
    "    def __init__(self, vocab_size, max_len, hidden_size, input_dropout_p, dropout_p, n_layers, rnn_cell):\n",
    "        super(BaseRNN, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_len = max_len\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.input_dropout_p = input_dropout_p\n",
    "        self.input_dropout = nn.Dropout(p=input_dropout_p)\n",
    "        if rnn_cell.lower() == 'lstm':\n",
    "            self.rnn_cell = nn.LSTM\n",
    "        elif rnn_cell.lower() == 'gru':\n",
    "            self.rnn_cell = nn.GRU\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported RNN Cell: {0}\".format(rnn_cell))\n",
    "\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(BaseRNN):\n",
    "    r\"\"\"\n",
    "    Applies a multi-layer RNN to an input sequence.\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int): size of the vocabulary\n",
    "        max_len (int): a maximum allowed length for the sequence to be processed\n",
    "        hidden_size (int): the number of features in the hidden state `h`\n",
    "        input_dropout_p (float, optional): dropout probability for the input sequence (default: 0)\n",
    "        dropout_p (float, optional): dropout probability for the output sequence (default: 0)\n",
    "        n_layers (int, optional): number of recurrent layers (default: 1)\n",
    "        bidirectional (bool, optional): if True, becomes a bidirectional encodr (defulat False)\n",
    "        rnn_cell (str, optional): type of RNN cell (default: gru)\n",
    "        variable_lengths (bool, optional): if use variable length RNN (default: False)\n",
    "        embedding (torch.Tensor, optional): Pre-trained embedding.  The size of the tensor has to match\n",
    "            the size of the embedding parameter: (vocab_size, hidden_size).  The embedding layer would be initialized\n",
    "            with the tensor if provided (default: None).\n",
    "        update_embedding (bool, optional): If the embedding should be updated during training (default: False).\n",
    "\n",
    "    Inputs: inputs, input_lengths\n",
    "        - **inputs**: list of sequences, whose length is the batch size and within which each sequence is a list of token IDs.\n",
    "        - **input_lengths** (list of int, optional): list that contains the lengths of sequences\n",
    "            in the mini-batch, it must be provided when using variable length RNN (default: `None`)\n",
    "\n",
    "    Outputs: output, hidden\n",
    "        - **output** (batch, seq_len, hidden_size): tensor containing the encoded features of the input sequence\n",
    "        - **hidden** (num_layers * num_directions, batch, hidden_size): tensor containing the features in the hidden state `h`\n",
    "\n",
    "    Examples::\n",
    "\n",
    "         >>> encoder = EncoderRNN(input_vocab, max_seq_length, hidden_size)\n",
    "         >>> output, hidden = encoder(input)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, feature_size, hidden_size,\n",
    "                 input_dropout_p=0, dropout_p=0,\n",
    "                 n_layers=6, bidirectional=False, rnn_cell='gru', variable_lengths=False):\n",
    "        super(EncoderRNN, self).__init__(0, 0, hidden_size,\n",
    "                input_dropout_p, dropout_p, n_layers, rnn_cell)\n",
    "\n",
    "        self.variable_lengths = variable_lengths\n",
    "        \n",
    "\n",
    "        \"\"\"\n",
    "        Copied from https://github.com/SeanNaren/deepspeech.pytorch/blob/master/model.py\n",
    "        Copyright (c) 2017 Sean Naren\n",
    "        MIT License\n",
    "        \"\"\"\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=(41, 11), stride=(2, 2), padding=(20, 5)),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Hardtanh(0, 20, inplace=True),\n",
    "            nn.Conv2d(32, 32, kernel_size=(21, 11), stride=(2, 1), padding=(10, 5)),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Hardtanh(0, 20, inplace=True)\n",
    "        )\n",
    "\n",
    "        feature_size = math.ceil((feature_size - 11 + 1 + (5*2)) / 2)\n",
    "        feature_size = math.ceil(feature_size - 11 + 1 + (5*2))\n",
    "        feature_size *= 32\n",
    "\n",
    "        self.rnn = self.rnn_cell(feature_size, hidden_size, n_layers,\n",
    "                                 batch_first=True, bidirectional=bidirectional, dropout=dropout_p)\n",
    "\n",
    "    def forward(self, input_var, input_lengths=None):\n",
    "        \"\"\"\n",
    "        Applies a multi-layer RNN to an input sequence.\n",
    "\n",
    "        Args:\n",
    "            input_var (batch, seq_len): tensor containing the features of the input sequence.\n",
    "            input_lengths (list of int, optional): A list that contains the lengths of sequences\n",
    "              in the mini-batch\n",
    "\n",
    "        Returns: output, hidden\n",
    "            - **output** (batch, seq_len, hidden_size): variable containing the encoded features of the input sequence\n",
    "            - **hidden** (num_layers * num_directions, batch, hidden_size): variable containing the features in the hidden state h\n",
    "        \"\"\"\n",
    "        \n",
    "        input_var = input_var.unsqueeze(1)\n",
    "        x = self.conv(input_var)\n",
    "\n",
    "        # BxCxTxD => BxCxDxT\n",
    "        x = x.transpose(1, 2)\n",
    "        x = x.contiguous()\n",
    "        sizes = x.size()\n",
    "        x = x.view(sizes[0], sizes[1], sizes[2] * sizes[3])\n",
    "\n",
    "        if self.training:\n",
    "            self.rnn.flatten_parameters()\n",
    "\n",
    "        output, hidden = self.rnn(x)\n",
    "\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(BaseRNN):\n",
    "    r\"\"\"\n",
    "    Provides functionality for decoding in a seq2seq framework, with an option for attention.\n",
    "    Args:\n",
    "        vocab_size (int): size of the vocabulary\n",
    "        max_len (int): a maximum allowed length for the sequence to be processed\n",
    "        hidden_size (int): the number of features in the hidden state `h`\n",
    "        sos_id (int): index of the start of sentence symbol\n",
    "        eos_id (int): index of the end of sentence symbol\n",
    "        n_layers (int, optional): number of recurrent layers (default: 1)\n",
    "        rnn_cell (str, optional): type of RNN cell (default: gru)\n",
    "        bidirectional (bool, optional): if the encoder is bidirectional (default False)\n",
    "        input_dropout_p (float, optional): dropout probability for the input sequence (default: 0)\n",
    "        dropout_p (float, optional): dropout probability for the output sequence (default: 0)\n",
    "        use_attention(bool, optional): flag indication whether to use attention mechanism or not (default: false)\n",
    "    Attributes:\n",
    "        KEY_ATTN_SCORE (str): key used to indicate attention weights in `ret_dict`\n",
    "        KEY_LENGTH (str): key used to indicate a list representing lengths of output sequences in `ret_dict`\n",
    "        KEY_SEQUENCE (str): key used to indicate a list of sequences in `ret_dict`\n",
    "    Inputs: inputs, encoder_hidden, encoder_outputs, function, teacher_forcing_ratio\n",
    "        - **inputs** (batch, seq_len, input_size): list of sequences, whose length is the batch size and within which\n",
    "          each sequence is a list of token IDs.  It is used for teacher forcing when provided. (default `None`)\n",
    "        - **encoder_hidden** (num_layers * num_directions, batch_size, hidden_size): tensor containing the features in the\n",
    "          hidden state `h` of encoder. Used as the initial hidden state of the decoder. (default `None`)\n",
    "        - **encoder_outputs** (batch, seq_len, hidden_size): tensor with containing the outputs of the encoder.\n",
    "          Used for attention mechanism (default is `None`).\n",
    "        - **function** (torch.nn.Module): A function used to generate symbols from RNN hidden state\n",
    "          (default is `torch.nn.functional.log_softmax`).\n",
    "        - **teacher_forcing_ratio** (float): The probability that teacher forcing will be used. A random number is\n",
    "          drawn uniformly from 0-1 for every decoding token, and if the sample is smaller than the given value,\n",
    "          teacher forcing would be used (default is 0).\n",
    "    Outputs: decoder_outputs, decoder_hidden, ret_dict\n",
    "        - **decoder_outputs** (seq_len, batch, vocab_size): list of tensors with size (batch_size, vocab_size) containing\n",
    "          the outputs of the decoding function.\n",
    "        - **decoder_hidden** (num_layers * num_directions, batch, hidden_size): tensor containing the last hidden\n",
    "          state of the decoder.\n",
    "        - **ret_dict**: dictionary containing additional information as follows {*KEY_LENGTH* : list of integers\n",
    "          representing lengths of output sequences, *KEY_SEQUENCE* : list of sequences, where each sequence is a list of\n",
    "          predicted token IDs }.\n",
    "    \"\"\"\n",
    "\n",
    "    KEY_ATTN_SCORE = 'attention_score'\n",
    "    KEY_LENGTH = 'length'\n",
    "    KEY_SEQUENCE = 'sequence'\n",
    "\n",
    "    def __init__(self, vocab_size, max_len, hidden_size,\n",
    "            sos_id, eos_id,\n",
    "            n_layers=1, rnn_cell='gru', bidirectional=False,\n",
    "            input_dropout_p=0, dropout_p=0, use_attention=False):\n",
    "        super(DecoderRNN, self).__init__(vocab_size, max_len, hidden_size,\n",
    "                input_dropout_p, dropout_p,\n",
    "                n_layers, rnn_cell)\n",
    "\n",
    "        self.bidirectional_encoder = bidirectional\n",
    "        self.rnn = self.rnn_cell(hidden_size, hidden_size, n_layers, batch_first=True, dropout=dropout_p)\n",
    "\n",
    "        self.output_size = vocab_size\n",
    "        self.max_length = max_len\n",
    "        self.use_attention = use_attention\n",
    "        self.eos_id = eos_id\n",
    "        self.sos_id = sos_id\n",
    "\n",
    "        self.init_input = None\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        if use_attention:\n",
    "            self.attention = Attention(self.hidden_size)\n",
    "\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward_step(self, input_var, hidden, encoder_outputs, function):\n",
    "        batch_size = input_var.size(0)\n",
    "        output_size = input_var.size(1)\n",
    "        embedded = self.embedding(input_var)\n",
    "        embedded = self.input_dropout(embedded)\n",
    "\n",
    "        if self.training:\n",
    "            self.rnn.flatten_parameters()\n",
    "\n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "\n",
    "        attn = None\n",
    "        if self.use_attention:\n",
    "            output, attn = self.attention(output, encoder_outputs)\n",
    "\n",
    "        predicted_softmax = function(self.out(output.contiguous().view(-1, self.hidden_size)), dim=1).view(batch_size, output_size, -1)\n",
    "        return predicted_softmax, hidden, attn\n",
    "\n",
    "    def forward(self, inputs=None, encoder_hidden=None, encoder_outputs=None,\n",
    "                    function=F.log_softmax, teacher_forcing_ratio=0):\n",
    "        ret_dict = dict()\n",
    "        if self.use_attention:\n",
    "            ret_dict[DecoderRNN.KEY_ATTN_SCORE] = list()\n",
    "\n",
    "        inputs, batch_size, max_length = self._validate_args(inputs, encoder_hidden, encoder_outputs,\n",
    "                                                             function, teacher_forcing_ratio)\n",
    "        decoder_hidden = self._init_state(encoder_hidden)\n",
    "\n",
    "        use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "        decoder_outputs = []\n",
    "        sequence_symbols = []\n",
    "        lengths = np.array([max_length] * batch_size)\n",
    "\n",
    "        def decode(step, step_output, step_attn):\n",
    "            decoder_outputs.append(step_output)\n",
    "            if self.use_attention:\n",
    "                ret_dict[DecoderRNN.KEY_ATTN_SCORE].append(step_attn)\n",
    "            symbols = decoder_outputs[-1].topk(1)[1]\n",
    "            sequence_symbols.append(symbols)\n",
    "\n",
    "            eos_batches = symbols.data.eq(self.eos_id)\n",
    "            if eos_batches.dim() > 0:\n",
    "                eos_batches = eos_batches.cpu().view(-1).numpy()\n",
    "                update_idx = ((lengths > step) & eos_batches) != 0\n",
    "                lengths[update_idx] = len(sequence_symbols)\n",
    "            return symbols\n",
    "\n",
    "        # Manual unrolling is used to support random teacher forcing.\n",
    "        # If teacher_forcing_ratio is True or False instead of a probability, the unrolling can be done in graph\n",
    "        if use_teacher_forcing:\n",
    "            decoder_input = inputs[:, :-1]\n",
    "            decoder_output, decoder_hidden, attn = self.forward_step(decoder_input, decoder_hidden, encoder_outputs,\n",
    "                                                                     function=function)\n",
    "\n",
    "            for di in range(decoder_output.size(1)):\n",
    "                step_output = decoder_output[:, di, :]\n",
    "                if attn is not None:\n",
    "                    step_attn = attn[:, di, :]\n",
    "                else:\n",
    "                    step_attn = None\n",
    "                decode(di, step_output, step_attn)\n",
    "        else:\n",
    "            decoder_input = inputs[:, 0].unsqueeze(1)\n",
    "            for di in range(max_length):\n",
    "                decoder_output, decoder_hidden, step_attn = self.forward_step(decoder_input, decoder_hidden, encoder_outputs,\n",
    "                                                                         function=function)\n",
    "                step_output = decoder_output.squeeze(1)\n",
    "                symbols = decode(di, step_output, step_attn)\n",
    "                decoder_input = symbols\n",
    "\n",
    "        ret_dict[DecoderRNN.KEY_SEQUENCE] = sequence_symbols\n",
    "        ret_dict[DecoderRNN.KEY_LENGTH] = lengths.tolist()\n",
    "\n",
    "        return decoder_outputs, decoder_hidden, ret_dict\n",
    "\n",
    "    def _init_state(self, encoder_hidden):\n",
    "        \"\"\" Initialize the encoder hidden state. \"\"\"\n",
    "        if encoder_hidden is None:\n",
    "            return None\n",
    "        if isinstance(encoder_hidden, tuple):\n",
    "            encoder_hidden = tuple([self._cat_directions(h) for h in encoder_hidden])\n",
    "        else:\n",
    "            encoder_hidden = self._cat_directions(encoder_hidden)\n",
    "        return encoder_hidden\n",
    "\n",
    "    def _cat_directions(self, h):\n",
    "        \"\"\" If the encoder is bidirectional, do the following transformation.\n",
    "            (#directions * #layers, #batch, hidden_size) -> (#layers, #batch, #directions * hidden_size)\n",
    "        \"\"\"\n",
    "        if self.bidirectional_encoder:\n",
    "            h = torch.cat([h[0:h.size(0):2], h[1:h.size(0):2]], 2)\n",
    "        return h\n",
    "\n",
    "    def _validate_args(self, inputs, encoder_hidden, encoder_outputs, function, teacher_forcing_ratio):\n",
    "        if self.use_attention:\n",
    "            if encoder_outputs is None:\n",
    "                raise ValueError(\"Argument encoder_outputs cannot be None when attention is used.\")\n",
    "\n",
    "        # inference batch size\n",
    "        if inputs is None and encoder_hidden is None:\n",
    "            batch_size = 1\n",
    "        else:\n",
    "            if inputs is not None:\n",
    "                batch_size = inputs.size(0)\n",
    "            else:\n",
    "                if self.rnn_cell is nn.LSTM:\n",
    "                    batch_size = encoder_hidden[0].size(1)\n",
    "                elif self.rnn_cell is nn.GRU:\n",
    "                    batch_size = encoder_hidden.size(1)\n",
    "\n",
    "        # set default input and max decoding length\n",
    "        if inputs is None:\n",
    "            if teacher_forcing_ratio > 0:\n",
    "                raise ValueError(\"Teacher forcing has to be disabled (set 0) when no inputs is provided.\")\n",
    "            inputs = torch.LongTensor([self.sos_id] * batch_size).view(batch_size, 1)\n",
    "            if torch.cuda.is_available():\n",
    "                inputs = inputs.cuda()\n",
    "            max_length = self.max_length\n",
    "        else:\n",
    "            max_length = inputs.size(1) - 1 # minus the start of sequence symbol\n",
    "\n",
    "        return inputs, batch_size, max_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    r\"\"\"\n",
    "    Applies an attention mechanism on the output features from the decoder.\n",
    "\n",
    "    .. math::\n",
    "            \\begin{array}{ll}\n",
    "            x = context*output \\\\\n",
    "            attn = exp(x_i) / sum_j exp(x_j) \\\\\n",
    "            output = \\tanh(w * (attn * context) + b * output)\n",
    "            \\end{array}\n",
    "\n",
    "    Args:\n",
    "        dim(int): The number of expected features in the output\n",
    "\n",
    "    Inputs: output, context\n",
    "        - **output** (batch, output_len, dimensions): tensor containing the output features from the decoder.\n",
    "        - **context** (batch, input_len, dimensions): tensor containing features of the encoded input sequence.\n",
    "\n",
    "    Outputs: output, attn\n",
    "        - **output** (batch, output_len, dimensions): tensor containing the attended output features from the decoder.\n",
    "        - **attn** (batch, output_len, input_len): tensor containing attention weights.\n",
    "\n",
    "    Attributes:\n",
    "        linear_out (torch.nn.Linear): applies a linear transformation to the incoming data: :math:`y = Ax + b`.\n",
    "        mask (torch.Tensor, optional): applies a :math:`-inf` to the indices specified in the `Tensor`.\n",
    "\n",
    "    Examples::\n",
    "\n",
    "         >>> attention = seq2seq.models.Attention(256)\n",
    "         >>> context = Variable(torch.randn(5, 3, 256))\n",
    "         >>> output = Variable(torch.randn(5, 5, 256))\n",
    "         >>> output, attn = attention(output, context)\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.linear_out = nn.Linear(dim*2, dim)\n",
    "        self.mask = None\n",
    "\n",
    "    def set_mask(self, mask):\n",
    "        \"\"\"\n",
    "        Sets indices to be masked\n",
    "\n",
    "        Args:\n",
    "            mask (torch.Tensor): tensor containing indices to be masked\n",
    "        \"\"\"\n",
    "        self.mask = mask\n",
    "\n",
    "    def forward(self, output, context):\n",
    "        batch_size = output.size(0)\n",
    "        hidden_size = output.size(2)\n",
    "        input_size = context.size(1)\n",
    "        # (batch, out_len, dim) * (batch, in_len, dim) -> (batch, out_len, in_len)\n",
    "        attn = torch.bmm(output, context.transpose(1, 2))\n",
    "        if self.mask is not None:\n",
    "            attn.data.masked_fill_(self.mask, -float('inf'))\n",
    "        attn = F.softmax(attn.view(-1, input_size), dim=1).view(batch_size, -1, input_size)\n",
    "\n",
    "        # (batch, out_len, in_len) * (batch, in_len, dim) -> (batch, out_len, dim)\n",
    "        mix = torch.bmm(attn, context)\n",
    "\n",
    "        # concat -> (batch, out_len, 2*dim)\n",
    "        combined = torch.cat((mix, output), dim=2)\n",
    "        # output -> (batch, out_len, dim)\n",
    "        output = torch.tanh(self.linear_out(combined.view(-1, 2 * hidden_size))).view(batch_size, -1, hidden_size)\n",
    "\n",
    "        return output, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2seq(nn.Module):\n",
    "    \"\"\" Standard sequence-to-sequence architecture with configurable encoder\n",
    "    and decoder.\n",
    "\n",
    "    Args:\n",
    "        encoder (EncoderRNN): object of EncoderRNN\n",
    "        decoder (DecoderRNN): object of DecoderRNN\n",
    "        decode_function (func, optional): function to generate symbols from output hidden states (default: F.log_softmax)\n",
    "\n",
    "    Inputs: input_variable, input_lengths, target_variable, teacher_forcing_ratio\n",
    "        - **input_variable** (list, option): list of sequences, whose length is the batch size and within which\n",
    "          each sequence is a list of token IDs. This information is forwarded to the encoder.\n",
    "        - **input_lengths** (list of int, optional): A list that contains the lengths of sequences\n",
    "            in the mini-batch, it must be provided when using variable length RNN (default: `None`)\n",
    "        - **target_variable** (list, optional): list of sequences, whose length is the batch size and within which\n",
    "          each sequence is a list of token IDs. This information is forwarded to the decoder.\n",
    "        - **teacher_forcing_ratio** (int, optional): The probability that teacher forcing will be used. A random number\n",
    "          is drawn uniformly from 0-1 for every decoding token, and if the sample is smaller than the given value,\n",
    "          teacher forcing would be used (default is 0)\n",
    "\n",
    "    Outputs: decoder_outputs, decoder_hidden, ret_dict\n",
    "        - **decoder_outputs** (batch): batch-length list of tensors with size (max_length, hidden_size) containing the\n",
    "          outputs of the decoder.\n",
    "        - **decoder_hidden** (num_layers * num_directions, batch, hidden_size): tensor containing the last hidden\n",
    "          state of the decoder.\n",
    "        - **ret_dict**: dictionary containing additional information as follows {*KEY_LENGTH* : list of integers\n",
    "          representing lengths of output sequences, *KEY_SEQUENCE* : list of sequences, where each sequence is a list of\n",
    "          predicted token IDs, *KEY_INPUT* : target outputs if provided for decoding, *KEY_ATTN_SCORE* : list of\n",
    "          sequences, where each list is of attention weights }.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder, decoder, decode_function=F.log_softmax):\n",
    "        super(Seq2seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.decode_function = decode_function\n",
    "\n",
    "    def flatten_parameters(self):\n",
    "        self.encoder.rnn.flatten_parameters()\n",
    "        self.decoder.rnn.flatten_parameters()\n",
    "\n",
    "    def forward(self, input_variable, input_lengths=None, target_variable=None,\n",
    "                teacher_forcing_ratio=0):\n",
    "\n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_variable, input_lengths)\n",
    "\n",
    "        result = self.decoder(inputs=target_variable,\n",
    "                              encoder_hidden=encoder_hidden,\n",
    "                              encoder_outputs=encoder_outputs,\n",
    "                              function=self.decode_function,\n",
    "                              teacher_forcing_ratio=teacher_forcing_ratio)\n",
    "        return result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_size = 40\n",
    "dropout = 0.2\n",
    "layer_size = 3\n",
    "hidden_size = 512\n",
    "max_len = 80\n",
    "\n",
    "enc = EncoderRNN(feature_size, hidden_size,\n",
    "                     input_dropout_p=dropout, dropout_p=dropout,\n",
    "                     n_layers=layer_size, bidirectional=True, rnn_cell='gru', variable_lengths=False)\n",
    "\n",
    "dec = DecoderRNN(len(CLASSES), max_len, hidden_size * (2 if True else 1), None, None,\n",
    "                     n_layers=layer_size, rnn_cell='gru', bidirectional=True,\n",
    "                     input_dropout_p=dropout, dropout_p=dropout, use_attention=True)\n",
    "\n",
    "model = Seq2seq(enc, dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34261292"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model = ResNet(BasicBlock, [2, 2, 2], num_classes=len(CLASSES), in_channels=1)\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arg():\n",
    "    def __init__(self):\n",
    "        self.train_dataset=\"/home/cilab/LabMembers/DJ/sr_dataset/speech_command/train\"\n",
    "        self.valid_dataset=\"/home/cilab/LabMembers/DJ/sr_dataset/speech_command/valid\"\n",
    "        self.background_noise=\"/home/cilab/LabMembers/DJ/sr_dataset/speech_command/train/_background_noise_\"\n",
    "        self.comment=\"\"\n",
    "        self.batch_size=64\n",
    "        self.dataload_workers_nums=6\n",
    "        self.weight_decay=1e-2\n",
    "        self.optim='sgd'\n",
    "        self.learning_rate=0.01\n",
    "        self.lr_scheduler='plateau'\n",
    "        self.lr_scheduler_patience=5\n",
    "        self.lr_scheduler_step_size=50\n",
    "        self.lr_scheduler_gamma=0.1\n",
    "        self.max_epochs=70\n",
    "        self.resume=None\n",
    "        self.model=\"resnet18\"\n",
    "        self.input=\"mel40\"\n",
    "        self.mixup=True\n",
    "args = Arg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_gpu True\n"
     ]
    }
   ],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "print('use_gpu', use_gpu)\n",
    "if use_gpu:\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "n_mels = 32\n",
    "if args.input == 'mel40':\n",
    "    n_mels = 40\n",
    "\n",
    "data_aug_transform = Compose([ChangeAmplitude(), ChangeSpeedAndPitchAudio(), FixAudioLength(), ToSTFT(), StretchAudioOnSTFT(), TimeshiftAudioOnSTFT(), FixSTFTDimension()])\n",
    "bg_dataset = BackgroundNoiseDataset(args.background_noise, data_aug_transform)\n",
    "add_bg_noise = AddBackgroundNoiseOnSTFT(bg_dataset)\n",
    "train_feature_transform = Compose([ToMelSpectrogramFromSTFT(n_mels=n_mels), DeleteSTFT(), ToTensor('mel_spectrogram', 'input')])\n",
    "train_dataset = SpeechCommandsDataset(args.train_dataset,\n",
    "                                Compose([LoadAudio(),\n",
    "                                         data_aug_transform,\n",
    "                                         add_bg_noise,\n",
    "                                         train_feature_transform]))\n",
    "\n",
    "valid_feature_transform = Compose([ToMelSpectrogram(n_mels=n_mels), ToTensor('mel_spectrogram', 'input')])\n",
    "valid_dataset = SpeechCommandsDataset(args.valid_dataset,\n",
    "                                Compose([LoadAudio(),\n",
    "                                         FixAudioLength(),\n",
    "                                         valid_feature_transform]))\n",
    "\n",
    "weights = train_dataset.make_weights_for_balanced_classes()\n",
    "sampler = WeightedRandomSampler(weights, len(weights))\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, sampler=sampler,\n",
    "                              pin_memory=use_gpu, num_workers=args.dataload_workers_nums)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=args.batch_size, shuffle=False,\n",
    "                              pin_memory=use_gpu, num_workers=args.dataload_workers_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(1, 12, kernel_size=(3, 3), stride=(1, 1), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=(4, 3), stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(12, 45, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(45, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(45, 45, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(45, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(12, 45, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(45, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(45, 45, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(45, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(45, 45, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(45, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(45, 45, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(45, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(45, 45, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(45, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(45, 45, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(45, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(45, 45, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(45, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(45, 45, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(45, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(45, 45, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(45, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(45, 45, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(45, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(45, 45, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(45, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(45, 45, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(45, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(45, 45, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(45, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(2, 1))\n",
      "  (fc): Linear(in_features=90, out_features=12, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# a name used to save checkpoints etc.\n",
    "full_name = '%s_%s_%s_bs%d_lr%.1e_wd%.1e' % (args.model, args.optim, args.lr_scheduler, args.batch_size, args.learning_rate, args.weight_decay)\n",
    "if args.comment:\n",
    "    full_name = '%s_%s' % (full_name, args.comment)\n",
    "\n",
    "#model = models.create_model(model_name=args.model, num_classes=len(CLASSES), in_channels=1)\n",
    "model = ResNet(BasicBlock, [2, 2, 2], num_classes=len(CLASSES), in_channels=1)\n",
    "print(model)\n",
    "if use_gpu:\n",
    "    model = torch.nn.DataParallel(model).cuda()\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "if args.optim == 'sgd':\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=args.learning_rate, momentum=0.9, weight_decay=args.weight_decay)\n",
    "else:\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "\n",
    "start_timestamp = int(time.time()*1000)\n",
    "start_epoch = 0\n",
    "best_accuracy = 0\n",
    "best_loss = 1e100\n",
    "global_step = 0\n",
    "\n",
    "if args.resume:\n",
    "    print(\"resuming a checkpoint '%s'\" % args.resume)\n",
    "    checkpoint = torch.load(args.resume)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    model.float()\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "    best_accuracy = checkpoint.get('accuracy', best_accuracy)\n",
    "    best_loss = checkpoint.get('loss', best_loss)\n",
    "    start_epoch = checkpoint.get('epoch', start_epoch)\n",
    "    global_step = checkpoint.get('step', global_step)\n",
    "\n",
    "    del checkpoint  # reduce memory\n",
    "\n",
    "if args.lr_scheduler == 'plateau':\n",
    "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=args.lr_scheduler_patience, factor=args.lr_scheduler_gamma)\n",
    "else:\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=args.lr_scheduler_step_size, gamma=args.lr_scheduler_gamma, last_epoch=start_epoch-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "212499"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr():\n",
    "    return optimizer.param_groups[0]['lr']\n",
    "\n",
    "writer = SummaryWriter(comment=('_speech_commands_' + full_name))\n",
    "\n",
    "def train(epoch):\n",
    "    global global_step\n",
    "\n",
    "    print(\"epoch %3d with lr=%.02e\" % (epoch, get_lr()))\n",
    "    phase = 'train'\n",
    "    writer.add_scalar('%s/learning_rate' % phase,  get_lr(), epoch)\n",
    "\n",
    "    model.train()  # Set model to training mode\n",
    "\n",
    "    running_loss = 0.0\n",
    "    it = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    pbar = tqdm(train_dataloader, unit=\"audios\", unit_scale=train_dataloader.batch_size)\n",
    "    for batch in pbar:\n",
    "        inputs = batch['input']\n",
    "        inputs = torch.unsqueeze(inputs, 1)\n",
    "        targets = batch['target']\n",
    "\n",
    "        if args.mixup:\n",
    "            inputs, targets = mixup(inputs, targets, num_classes=len(CLASSES))\n",
    "\n",
    "        inputs = Variable(inputs, requires_grad=True)\n",
    "        targets = Variable(targets, requires_grad=False)\n",
    "        if use_gpu:\n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda(async=True)\n",
    "\n",
    "        # forward/backward\n",
    "        outputs = model(inputs)\n",
    "        if args.mixup:\n",
    "            loss = mixup_cross_entropy_loss(outputs, targets)\n",
    "        else:\n",
    "            loss = criterion(outputs, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # statistics\n",
    "        it += 1\n",
    "        global_step += 1\n",
    "        running_loss += loss.item()\n",
    "        pred = outputs.data.max(1, keepdim=True)[1]\n",
    "        if args.mixup:\n",
    "            targets = batch['target']\n",
    "            targets = Variable(targets, requires_grad=False).cuda(async=True)\n",
    "        correct += pred.eq(targets.data.view_as(pred)).sum()\n",
    "        total += targets.size(0)\n",
    "\n",
    "        writer.add_scalar('%s/loss' % phase, loss.item(), global_step)\n",
    "\n",
    "        # update the progress bar\n",
    "        pbar.set_postfix({\n",
    "            'loss': \"%.05f\" % (running_loss / it),\n",
    "            'acc': \"%.02f%%\" % (100*correct/total)\n",
    "        })\n",
    "\n",
    "    accuracy = correct/total\n",
    "    epoch_loss = running_loss / it\n",
    "    writer.add_scalar('%s/accuracy' % phase, 100*accuracy, epoch)\n",
    "    writer.add_scalar('%s/epoch_loss' % phase, epoch_loss, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(epoch):\n",
    "    global best_accuracy, best_loss, global_step\n",
    "\n",
    "    phase = 'valid'\n",
    "    model.eval()  # Set model to evaluate mode\n",
    "\n",
    "    running_loss = 0.0\n",
    "    it = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    pbar = tqdm(valid_dataloader, unit=\"audios\", unit_scale=valid_dataloader.batch_size)\n",
    "    for batch in pbar:\n",
    "        inputs = batch['input']\n",
    "        inputs = torch.unsqueeze(inputs, 1)\n",
    "        targets = batch['target']\n",
    "\n",
    "        inputs = Variable(inputs, volatile = True)\n",
    "        targets = Variable(targets, requires_grad=False)\n",
    "\n",
    "        if use_gpu:\n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda(async=True)\n",
    "\n",
    "        # forward\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # statistics\n",
    "        it += 1\n",
    "        global_step += 1\n",
    "        running_loss += loss.item()\n",
    "        pred = outputs.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(targets.data.view_as(pred)).sum()\n",
    "        total += targets.size(0)\n",
    "\n",
    "        writer.add_scalar('%s/loss' % phase, loss.item(), global_step)\n",
    "\n",
    "        # update the progress bar\n",
    "        pbar.set_postfix({\n",
    "            'loss': \"%.05f\" % (running_loss / it),\n",
    "            'acc': \"%.02f%%\" % (100*correct/total)\n",
    "        })\n",
    "\n",
    "    accuracy = 100*correct/total\n",
    "    epoch_loss = running_loss / it\n",
    "    writer.add_scalar('%s/accuracy' % phase, accuracy, epoch)\n",
    "    writer.add_scalar('%s/epoch_loss' % phase, epoch_loss, epoch)\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'step': global_step,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'loss': epoch_loss,\n",
    "        'accuracy': accuracy,\n",
    "        'optimizer' : optimizer.state_dict(),\n",
    "    }\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(checkpoint, 'checkpoints/best-acc-resnet18-%s.pth' % full_name)\n",
    "        torch.save(model, '%d-%s-best-loss.pth' % (start_timestamp, full_name))\n",
    "    if epoch_loss < best_loss:\n",
    "        best_loss = epoch_loss\n",
    "        torch.save(checkpoint, 'checkpoints/best-loss-resnet18-%s.pth' % full_name)\n",
    "        torch.save(model, '%d-%s-best-acc.pth' % (start_timestamp, full_name))\n",
    "    torch.save(model, './res18.pth')\n",
    "    #torch.save(checkpoint, 'checkpoints/Resnet18.pth')\n",
    "    del checkpoint  # reduce memory\n",
    "\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/56256 [00:00<?, ?audios/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training resnet18 for Google speech commands...\n",
      "epoch   0 with lr=1.00e-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56256/56256 [02:52<00:00, 326.35audios/s, loss=2.04352, acc=27.00%]\n",
      "  0%|          | 0/7488 [00:00<?, ?audios/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "100%|██████████| 7488/7488 [00:06<00:00, 1162.87audios/s, loss=1.01833, acc=63.00%]\n",
      "/home/cilab/.local/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DataParallel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/cilab/.local/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ResNet. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/cilab/.local/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Conv2d. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/cilab/.local/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BatchNorm2d. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/cilab/.local/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/cilab/.local/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type MaxPool2d. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/cilab/.local/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Sequential. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/cilab/.local/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BasicBlock. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/cilab/.local/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type AdaptiveAvgPool2d. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/cilab/.local/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "  0%|          | 0/56256 [00:00<?, ?audios/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "total time elapsed: 0h 2m 59s , best accuracy: 63.00%, best loss 1.018326\n",
      "epoch   1 with lr=1.00e-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56256/56256 [02:36<00:00, 359.45audios/s, loss=1.65780, acc=41.00%]\n",
      "100%|██████████| 7488/7488 [00:06<00:00, 1152.37audios/s, loss=0.75405, acc=78.00%]\n",
      "  0%|          | 0/56256 [00:00<?, ?audios/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "total time elapsed: 0h 5m 42s , best accuracy: 78.00%, best loss 0.754048\n",
      "epoch   2 with lr=1.00e-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56256/56256 [02:34<00:00, 363.99audios/s, loss=1.59521, acc=42.00%]\n",
      "100%|██████████| 7488/7488 [00:06<00:00, 1072.13audios/s, loss=1.39435, acc=55.00%]\n",
      "  0%|          | 0/56256 [00:00<?, ?audios/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "total time elapsed: 0h 8m 24s , best accuracy: 78.00%, best loss 0.754048\n",
      "epoch   3 with lr=1.00e-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56256/56256 [02:36<00:00, 359.81audios/s, loss=1.57956, acc=43.00%]\n",
      "100%|██████████| 7488/7488 [00:06<00:00, 1157.94audios/s, loss=1.13632, acc=65.00%]\n",
      "  0%|          | 0/56256 [00:00<?, ?audios/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "total time elapsed: 0h 11m 6s , best accuracy: 78.00%, best loss 0.754048\n",
      "epoch   4 with lr=1.00e-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56256/56256 [02:37<00:00, 357.48audios/s, loss=1.55939, acc=43.00%]\n",
      "100%|██████████| 7488/7488 [00:06<00:00, 1086.84audios/s, loss=1.65950, acc=46.00%]\n",
      "  0%|          | 0/56256 [00:00<?, ?audios/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "total time elapsed: 0h 13m 51s , best accuracy: 78.00%, best loss 0.754048\n",
      "epoch   5 with lr=1.00e-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56256/56256 [02:37<00:00, 357.76audios/s, loss=1.55636, acc=44.00%]\n",
      "100%|██████████| 7488/7488 [00:06<00:00, 1158.50audios/s, loss=0.69755, acc=82.00%]\n",
      "  0%|          | 0/56256 [00:00<?, ?audios/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "total time elapsed: 0h 16m 34s , best accuracy: 82.00%, best loss 0.697550\n",
      "epoch   6 with lr=1.00e-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56256/56256 [02:37<00:00, 356.97audios/s, loss=1.53871, acc=44.00%]\n",
      "100%|██████████| 7488/7488 [00:06<00:00, 1193.29audios/s, loss=1.43511, acc=50.00%]\n",
      "  0%|          | 0/56256 [00:00<?, ?audios/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "total time elapsed: 0h 19m 18s , best accuracy: 82.00%, best loss 0.697550\n",
      "epoch   7 with lr=1.00e-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56256/56256 [02:36<00:00, 358.74audios/s, loss=1.53517, acc=44.00%]\n",
      "100%|██████████| 7488/7488 [00:06<00:00, 1190.53audios/s, loss=0.92779, acc=70.00%]\n",
      "  0%|          | 0/56256 [00:00<?, ?audios/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "total time elapsed: 0h 22m 1s , best accuracy: 82.00%, best loss 0.697550\n",
      "epoch   8 with lr=1.00e-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56256/56256 [02:39<00:00, 353.00audios/s, loss=1.53722, acc=44.00%]\n",
      "100%|██████████| 7488/7488 [00:06<00:00, 1096.28audios/s, loss=0.75551, acc=79.00%]\n",
      "  0%|          | 0/56256 [00:00<?, ?audios/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "total time elapsed: 0h 24m 48s , best accuracy: 82.00%, best loss 0.697550\n",
      "epoch   9 with lr=1.00e-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56256/56256 [01:30<00:00, 623.15audios/s, loss=1.53174, acc=44.00%] \n",
      "100%|██████████| 7488/7488 [00:04<00:00, 1725.71audios/s, loss=0.53771, acc=86.00%]\n",
      "  0%|          | 0/56256 [00:00<?, ?audios/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "total time elapsed: 0h 26m 22s , best accuracy: 86.00%, best loss 0.537706\n",
      "epoch  10 with lr=1.00e-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56256/56256 [01:25<00:00, 657.49audios/s, loss=1.52925, acc=44.00%] \n",
      "100%|██████████| 7488/7488 [00:04<00:00, 1723.87audios/s, loss=0.76026, acc=79.00%]\n",
      "  0%|          | 0/56256 [00:00<?, ?audios/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "total time elapsed: 0h 27m 52s , best accuracy: 86.00%, best loss 0.537706\n",
      "epoch  11 with lr=1.00e-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56256/56256 [01:58<00:00, 476.40audios/s, loss=1.52635, acc=45.00%]\n",
      "100%|██████████| 7488/7488 [00:05<00:00, 1419.59audios/s, loss=0.93481, acc=69.00%]\n",
      "  0%|          | 0/56256 [00:00<?, ?audios/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "total time elapsed: 0h 29m 56s , best accuracy: 86.00%, best loss 0.537706\n",
      "epoch  12 with lr=1.00e-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56256/56256 [01:23<00:00, 676.10audios/s, loss=1.51975, acc=45.00%] \n",
      "100%|██████████| 7488/7488 [00:04<00:00, 1670.57audios/s, loss=0.69655, acc=77.00%]\n",
      "  0%|          | 0/56256 [00:00<?, ?audios/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "total time elapsed: 0h 31m 23s , best accuracy: 86.00%, best loss 0.537706\n",
      "epoch  13 with lr=1.00e-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56256/56256 [01:24<00:00, 669.41audios/s, loss=1.52349, acc=44.00%] \n",
      "100%|██████████| 7488/7488 [00:04<00:00, 1654.11audios/s, loss=0.66903, acc=80.00%]\n",
      "  0%|          | 0/56256 [00:00<?, ?audios/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "total time elapsed: 0h 32m 52s , best accuracy: 86.00%, best loss 0.537706\n",
      "epoch  14 with lr=1.00e-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56256/56256 [01:24<00:00, 665.46audios/s, loss=1.53275, acc=44.00%] \n",
      "100%|██████████| 7488/7488 [00:04<00:00, 1743.83audios/s, loss=1.31305, acc=56.00%]\n",
      "  0%|          | 0/56256 [00:00<?, ?audios/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "total time elapsed: 0h 34m 21s , best accuracy: 86.00%, best loss 0.537706\n",
      "epoch  15 with lr=1.00e-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56256/56256 [01:25<00:00, 661.60audios/s, loss=1.52363, acc=44.00%] \n",
      "100%|██████████| 7488/7488 [00:04<00:00, 1662.25audios/s, loss=0.85620, acc=74.00%]\n",
      "  0%|          | 0/56256 [00:00<?, ?audios/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "total time elapsed: 0h 35m 50s , best accuracy: 86.00%, best loss 0.537706\n",
      "epoch  16 with lr=1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56256/56256 [01:24<00:00, 669.71audios/s, loss=1.38431, acc=48.00%]\n",
      "100%|██████████| 7488/7488 [00:04<00:00, 1778.32audios/s, loss=0.42115, acc=88.00%]\n",
      "  0%|          | 0/56256 [00:00<?, ?audios/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "total time elapsed: 0h 37m 19s , best accuracy: 88.00%, best loss 0.421149\n",
      "epoch  17 with lr=1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56256/56256 [01:25<00:00, 657.59audios/s, loss=1.34799, acc=48.00%]\n",
      "100%|██████████| 7488/7488 [00:04<00:00, 1721.70audios/s, loss=0.44888, acc=89.00%]\n",
      "  0%|          | 0/56256 [00:00<?, ?audios/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "total time elapsed: 0h 38m 49s , best accuracy: 89.00%, best loss 0.421149\n",
      "epoch  18 with lr=1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56256/56256 [01:23<00:00, 673.34audios/s, loss=1.33398, acc=48.00%] \n",
      "100%|██████████| 7488/7488 [00:04<00:00, 1754.66audios/s, loss=0.35310, acc=91.00%]\n",
      "  0%|          | 0/56256 [00:00<?, ?audios/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "total time elapsed: 0h 40m 17s , best accuracy: 91.00%, best loss 0.353100\n",
      "epoch  19 with lr=1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56256/56256 [01:23<00:00, 671.83audios/s, loss=1.32435, acc=49.00%]\n",
      "100%|██████████| 7488/7488 [00:04<00:00, 1689.68audios/s, loss=0.42009, acc=88.00%]\n",
      "  0%|          | 0/56256 [00:00<?, ?audios/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "total time elapsed: 0h 41m 45s , best accuracy: 91.00%, best loss 0.353100\n",
      "epoch  20 with lr=1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56256/56256 [01:22<00:00, 681.23audios/s, loss=1.31996, acc=49.00%]\n",
      "100%|██████████| 7488/7488 [00:04<00:00, 1691.64audios/s, loss=0.33120, acc=92.00%]\n",
      "  0%|          | 0/56256 [00:00<?, ?audios/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "total time elapsed: 0h 43m 12s , best accuracy: 92.00%, best loss 0.331199\n",
      "epoch  21 with lr=1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56256/56256 [01:25<00:00, 660.97audios/s, loss=1.30978, acc=49.00%] \n",
      "100%|██████████| 7488/7488 [00:04<00:00, 1728.62audios/s, loss=0.42631, acc=89.00%]\n",
      "  0%|          | 0/56256 [00:00<?, ?audios/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "total time elapsed: 0h 44m 41s , best accuracy: 92.00%, best loss 0.331199\n",
      "epoch  22 with lr=1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56256/56256 [01:25<00:00, 657.47audios/s, loss=1.32014, acc=48.00%]\n",
      "100%|██████████| 7488/7488 [00:04<00:00, 1754.15audios/s, loss=0.28769, acc=93.00%]\n",
      "  0%|          | 0/56256 [00:00<?, ?audios/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "total time elapsed: 0h 46m 11s , best accuracy: 93.00%, best loss 0.287688\n",
      "epoch  23 with lr=1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56256/56256 [01:23<00:00, 677.30audios/s, loss=1.31315, acc=49.00%]\n",
      "100%|██████████| 7488/7488 [00:04<00:00, 1702.63audios/s, loss=0.29181, acc=93.00%]\n",
      "  0%|          | 0/56256 [00:00<?, ?audios/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "total time elapsed: 0h 47m 39s , best accuracy: 93.00%, best loss 0.287688\n",
      "epoch  24 with lr=1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56256/56256 [01:23<00:00, 670.80audios/s, loss=1.30341, acc=49.00%]\n",
      "100%|██████████| 7488/7488 [00:04<00:00, 1509.50audios/s, loss=0.41337, acc=89.00%]\n",
      "  0%|          | 0/56256 [00:00<?, ?audios/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "total time elapsed: 0h 49m 8s , best accuracy: 93.00%, best loss 0.287688\n",
      "epoch  25 with lr=1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56256/56256 [01:23<00:00, 671.92audios/s, loss=1.30761, acc=49.00%]\n",
      "100%|██████████| 7488/7488 [00:04<00:00, 1742.51audios/s, loss=0.39451, acc=89.00%]\n",
      "  0%|          | 0/56256 [00:00<?, ?audios/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "total time elapsed: 0h 50m 36s , best accuracy: 93.00%, best loss 0.287688\n",
      "epoch  26 with lr=1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56256/56256 [01:22<00:00, 678.13audios/s, loss=1.29883, acc=49.00%]\n",
      "100%|██████████| 7488/7488 [00:04<00:00, 1661.24audios/s, loss=0.42636, acc=89.00%]\n",
      "  0%|          | 0/56256 [00:00<?, ?audios/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "total time elapsed: 0h 52m 3s , best accuracy: 93.00%, best loss 0.287688\n",
      "epoch  27 with lr=1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56256/56256 [01:23<00:00, 675.65audios/s, loss=1.30011, acc=49.00%]\n",
      "100%|██████████| 7488/7488 [00:04<00:00, 1684.49audios/s, loss=0.35392, acc=91.00%]\n",
      "  0%|          | 0/56256 [00:00<?, ?audios/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "total time elapsed: 0h 53m 31s , best accuracy: 93.00%, best loss 0.287688\n",
      "epoch  28 with lr=1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56256/56256 [01:22<00:00, 677.87audios/s, loss=1.29159, acc=49.00%] \n",
      "100%|██████████| 7488/7488 [00:04<00:00, 1681.94audios/s, loss=0.35021, acc=92.00%]\n",
      "  0%|          | 0/56256 [00:00<?, ?audios/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "total time elapsed: 0h 54m 58s , best accuracy: 93.00%, best loss 0.287688\n",
      "epoch  29 with lr=1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56256/56256 [01:24<00:00, 663.99audios/s, loss=1.24621, acc=49.00%]\n",
      "100%|██████████| 7488/7488 [00:04<00:00, 1661.10audios/s, loss=0.28853, acc=93.00%]\n",
      "  0%|          | 0/56256 [00:00<?, ?audios/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "total time elapsed: 0h 56m 28s , best accuracy: 93.00%, best loss 0.287688\n",
      "epoch  30 with lr=1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56256/56256 [01:25<00:00, 659.88audios/s, loss=1.22335, acc=50.00%] \n",
      "100%|██████████| 7488/7488 [00:04<00:00, 1759.50audios/s, loss=0.27962, acc=93.00%]\n",
      "  0%|          | 0/56256 [00:00<?, ?audios/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "total time elapsed: 0h 57m 57s , best accuracy: 93.00%, best loss 0.279624\n",
      "epoch  31 with lr=1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56256/56256 [01:25<00:00, 656.24audios/s, loss=1.21779, acc=51.00%] \n",
      "100%|██████████| 7488/7488 [00:04<00:00, 1775.35audios/s, loss=0.27557, acc=94.00%]\n",
      "  0%|          | 0/56256 [00:00<?, ?audios/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "total time elapsed: 0h 59m 27s , best accuracy: 94.00%, best loss 0.275569\n",
      "epoch  32 with lr=1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56256/56256 [01:22<00:00, 678.97audios/s, loss=1.20928, acc=50.00%]\n",
      "100%|██████████| 7488/7488 [00:04<00:00, 1708.50audios/s, loss=0.26231, acc=94.00%]\n",
      "  0%|          | 0/56256 [00:00<?, ?audios/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "total time elapsed: 1h 0m 54s , best accuracy: 94.00%, best loss 0.262310\n",
      "epoch  33 with lr=1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56256/56256 [01:22<00:00, 679.85audios/s, loss=1.20756, acc=50.00%] \n",
      "100%|██████████| 7488/7488 [00:04<00:00, 1700.37audios/s, loss=0.31575, acc=93.00%]\n",
      "  0%|          | 0/56256 [00:00<?, ?audios/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "total time elapsed: 1h 2m 22s , best accuracy: 94.00%, best loss 0.262310\n",
      "epoch  34 with lr=1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56256/56256 [01:24<00:00, 665.27audios/s, loss=1.20463, acc=50.00%]\n",
      "100%|██████████| 7488/7488 [00:04<00:00, 1744.01audios/s, loss=0.27235, acc=94.00%]\n",
      "  0%|          | 0/56256 [00:00<?, ?audios/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "total time elapsed: 1h 3m 50s , best accuracy: 94.00%, best loss 0.262310\n",
      "epoch  35 with lr=1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56256/56256 [01:23<00:00, 674.67audios/s, loss=1.19860, acc=50.00%] \n",
      "100%|██████████| 7488/7488 [00:04<00:00, 1789.46audios/s, loss=0.26294, acc=94.00%]\n",
      "  0%|          | 0/56256 [00:00<?, ?audios/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "total time elapsed: 1h 5m 18s , best accuracy: 94.00%, best loss 0.262310\n",
      "epoch  36 with lr=1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56256/56256 [01:23<00:00, 672.61audios/s, loss=1.19890, acc=50.00%] \n",
      "100%|██████████| 7488/7488 [00:04<00:00, 1693.79audios/s, loss=0.28673, acc=94.00%]\n",
      "  0%|          | 0/56256 [00:00<?, ?audios/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "total time elapsed: 1h 6m 46s , best accuracy: 94.00%, best loss 0.262310\n",
      "epoch  37 with lr=1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56256/56256 [01:22<00:00, 679.43audios/s, loss=1.19683, acc=50.00%]\n",
      "100%|██████████| 7488/7488 [00:04<00:00, 1690.88audios/s, loss=0.27033, acc=94.00%]\n",
      "  0%|          | 0/56256 [00:00<?, ?audios/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "total time elapsed: 1h 8m 13s , best accuracy: 94.00%, best loss 0.262310\n",
      "epoch  38 with lr=1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56256/56256 [01:22<00:00, 678.73audios/s, loss=1.19191, acc=51.00%] \n",
      "100%|██████████| 7488/7488 [00:04<00:00, 1665.72audios/s, loss=0.27086, acc=94.00%]\n",
      "  0%|          | 0/56256 [00:00<?, ?audios/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "total time elapsed: 1h 9m 41s , best accuracy: 94.00%, best loss 0.262310\n",
      "epoch  39 with lr=1.00e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56256/56256 [01:22<00:00, 682.77audios/s, loss=1.18095, acc=50.00%]\n",
      "100%|██████████| 7488/7488 [00:04<00:00, 1666.54audios/s, loss=0.26159, acc=94.00%]\n",
      "  0%|          | 0/56256 [00:00<?, ?audios/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "total time elapsed: 1h 11m 8s , best accuracy: 94.00%, best loss 0.261585\n",
      "epoch  40 with lr=1.00e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56256/56256 [01:23<00:00, 676.57audios/s, loss=1.17899, acc=51.00%]\n",
      "100%|██████████| 7488/7488 [00:04<00:00, 1552.16audios/s, loss=0.24389, acc=95.00%]\n",
      "  0%|          | 0/56256 [00:00<?, ?audios/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "total time elapsed: 1h 12m 36s , best accuracy: 95.00%, best loss 0.243893\n",
      "epoch  41 with lr=1.00e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56256/56256 [01:25<00:00, 654.15audios/s, loss=1.18075, acc=51.00%]\n",
      "100%|██████████| 7488/7488 [00:04<00:00, 1778.43audios/s, loss=0.24863, acc=95.00%]\n",
      "  0%|          | 0/56256 [00:00<?, ?audios/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "total time elapsed: 1h 14m 6s , best accuracy: 95.00%, best loss 0.243893\n",
      "epoch  42 with lr=1.00e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56256/56256 [01:26<00:00, 650.18audios/s, loss=1.17917, acc=51.00%] \n",
      "100%|██████████| 7488/7488 [00:04<00:00, 1720.21audios/s, loss=0.24400, acc=95.00%]\n",
      "  0%|          | 0/56256 [00:00<?, ?audios/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "total time elapsed: 1h 15m 37s , best accuracy: 95.00%, best loss 0.243893\n",
      "epoch  43 with lr=1.00e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56256/56256 [01:25<00:00, 657.01audios/s, loss=1.17453, acc=51.00%] \n",
      "100%|██████████| 7488/7488 [00:04<00:00, 1699.16audios/s, loss=0.25274, acc=95.00%]\n",
      "  0%|          | 0/56256 [00:00<?, ?audios/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "total time elapsed: 1h 17m 7s , best accuracy: 95.00%, best loss 0.243893\n",
      "epoch  44 with lr=1.00e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56256/56256 [01:24<00:00, 668.97audios/s, loss=1.18000, acc=51.00%]\n",
      "100%|██████████| 7488/7488 [00:04<00:00, 1704.17audios/s, loss=0.24929, acc=94.00%]\n",
      "  0%|          | 0/56256 [00:00<?, ?audios/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "total time elapsed: 1h 18m 36s , best accuracy: 95.00%, best loss 0.243893\n",
      "epoch  45 with lr=1.00e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56256/56256 [01:24<00:00, 668.15audios/s, loss=1.18097, acc=50.00%]\n",
      "100%|██████████| 7488/7488 [00:04<00:00, 1685.58audios/s, loss=0.26353, acc=95.00%]\n",
      "  0%|          | 0/56256 [00:00<?, ?audios/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "total time elapsed: 1h 20m 4s , best accuracy: 95.00%, best loss 0.243893\n",
      "epoch  46 with lr=1.00e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56256/56256 [01:24<00:00, 662.59audios/s, loss=1.17275, acc=51.00%] \n",
      "100%|██████████| 7488/7488 [00:04<00:00, 1577.24audios/s, loss=0.24966, acc=95.00%]\n",
      "  0%|          | 0/56256 [00:00<?, ?audios/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "total time elapsed: 1h 21m 34s , best accuracy: 95.00%, best loss 0.243893\n",
      "epoch  47 with lr=1.00e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56256/56256 [01:22<00:00, 682.35audios/s, loss=1.17827, acc=51.00%]\n",
      "100%|██████████| 7488/7488 [00:04<00:00, 1689.75audios/s, loss=0.24093, acc=95.00%]\n",
      "  0%|          | 0/56256 [00:00<?, ?audios/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "total time elapsed: 1h 23m 1s , best accuracy: 95.00%, best loss 0.240934\n",
      "epoch  48 with lr=1.00e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56256/56256 [01:23<00:00, 675.73audios/s, loss=1.17517, acc=51.00%]\n",
      "100%|██████████| 7488/7488 [00:04<00:00, 1681.56audios/s, loss=0.25007, acc=95.00%]\n",
      "  0%|          | 0/56256 [00:00<?, ?audios/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "total time elapsed: 1h 24m 29s , best accuracy: 95.00%, best loss 0.240934\n",
      "epoch  49 with lr=1.00e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56256/56256 [01:24<00:00, 666.49audios/s, loss=1.17806, acc=51.00%] \n",
      "100%|██████████| 7488/7488 [00:04<00:00, 1719.46audios/s, loss=0.25075, acc=94.00%]\n",
      "  0%|          | 0/56256 [00:00<?, ?audios/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "total time elapsed: 1h 25m 57s , best accuracy: 95.00%, best loss 0.240934\n",
      "epoch  50 with lr=1.00e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56256/56256 [01:22<00:00, 680.79audios/s, loss=1.18004, acc=51.00%]\n",
      "100%|██████████| 7488/7488 [00:04<00:00, 1693.72audios/s, loss=0.24340, acc=95.00%]\n",
      "  0%|          | 0/56256 [00:00<?, ?audios/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "total time elapsed: 1h 27m 24s , best accuracy: 95.00%, best loss 0.240934\n",
      "epoch  51 with lr=1.00e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56256/56256 [01:23<00:00, 672.43audios/s, loss=1.17776, acc=50.00%] \n",
      "100%|██████████| 7488/7488 [00:04<00:00, 1758.23audios/s, loss=0.24216, acc=94.00%]\n",
      "  0%|          | 0/56256 [00:00<?, ?audios/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "total time elapsed: 1h 28m 52s , best accuracy: 95.00%, best loss 0.240934\n",
      "epoch  52 with lr=1.00e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56256/56256 [01:22<00:00, 678.22audios/s, loss=1.17721, acc=51.00%]\n",
      "100%|██████████| 7488/7488 [00:04<00:00, 1729.80audios/s, loss=0.24149, acc=95.00%]\n",
      "  0%|          | 0/56256 [00:00<?, ?audios/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "total time elapsed: 1h 30m 20s , best accuracy: 95.00%, best loss 0.240934\n",
      "epoch  53 with lr=1.00e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56256/56256 [01:22<00:00, 678.68audios/s, loss=1.17313, acc=51.00%] \n",
      "100%|██████████| 7488/7488 [00:04<00:00, 1750.33audios/s, loss=0.26964, acc=94.00%]\n",
      "  0%|          | 0/56256 [00:00<?, ?audios/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "total time elapsed: 1h 31m 47s , best accuracy: 95.00%, best loss 0.240934\n",
      "epoch  54 with lr=1.00e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56256/56256 [01:23<00:00, 674.34audios/s, loss=1.17191, acc=51.00%]\n",
      "100%|██████████| 7488/7488 [00:04<00:00, 1746.02audios/s, loss=0.27740, acc=94.00%]\n",
      "  0%|          | 0/56256 [00:00<?, ?audios/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "total time elapsed: 1h 33m 15s , best accuracy: 95.00%, best loss 0.240934\n",
      "epoch  55 with lr=1.00e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56256/56256 [01:23<00:00, 674.36audios/s, loss=1.17679, acc=51.00%] \n",
      "100%|██████████| 7488/7488 [00:04<00:00, 1709.41audios/s, loss=0.22443, acc=95.00%]\n",
      "  0%|          | 0/56256 [00:00<?, ?audios/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "total time elapsed: 1h 34m 43s , best accuracy: 95.00%, best loss 0.224434\n",
      "epoch  56 with lr=1.00e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56256/56256 [01:23<00:00, 677.37audios/s, loss=1.17594, acc=50.00%] \n",
      "100%|██████████| 7488/7488 [00:04<00:00, 1773.22audios/s, loss=0.26107, acc=95.00%]\n",
      "  0%|          | 0/56256 [00:00<?, ?audios/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "total time elapsed: 1h 36m 10s , best accuracy: 95.00%, best loss 0.224434\n",
      "epoch  57 with lr=1.00e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56256/56256 [01:23<00:00, 676.15audios/s, loss=1.17682, acc=51.00%]\n",
      "100%|██████████| 7488/7488 [00:04<00:00, 1663.60audios/s, loss=0.24672, acc=95.00%]\n",
      "  0%|          | 0/56256 [00:00<?, ?audios/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "total time elapsed: 1h 37m 38s , best accuracy: 95.00%, best loss 0.224434\n",
      "epoch  58 with lr=1.00e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56256/56256 [01:23<00:00, 676.49audios/s, loss=1.17091, acc=51.00%]\n",
      "100%|██████████| 7488/7488 [00:04<00:00, 1691.92audios/s, loss=0.28404, acc=94.00%]\n",
      "  0%|          | 0/56256 [00:00<?, ?audios/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "total time elapsed: 1h 39m 5s , best accuracy: 95.00%, best loss 0.224434\n",
      "epoch  59 with lr=1.00e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56256/56256 [01:23<00:00, 676.43audios/s, loss=1.17327, acc=51.00%]\n",
      "100%|██████████| 7488/7488 [00:04<00:00, 1756.19audios/s, loss=0.23221, acc=95.00%]\n",
      "  0%|          | 0/56256 [00:00<?, ?audios/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "total time elapsed: 1h 40m 33s , best accuracy: 95.00%, best loss 0.224434\n",
      "epoch  60 with lr=1.00e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56256/56256 [01:22<00:00, 679.43audios/s, loss=1.17898, acc=51.00%]\n",
      "100%|██████████| 7488/7488 [00:04<00:00, 1829.75audios/s, loss=0.25639, acc=95.00%]\n",
      "  0%|          | 0/56256 [00:00<?, ?audios/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "total time elapsed: 1h 41m 60s , best accuracy: 95.00%, best loss 0.224434\n",
      "epoch  61 with lr=1.00e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56256/56256 [01:23<00:00, 674.97audios/s, loss=1.17577, acc=50.00%]\n",
      "100%|██████████| 7488/7488 [00:04<00:00, 1681.50audios/s, loss=0.25425, acc=95.00%]\n",
      "  0%|          | 0/56256 [00:00<?, ?audios/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "total time elapsed: 1h 43m 27s , best accuracy: 95.00%, best loss 0.224434\n",
      "epoch  62 with lr=1.00e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56256/56256 [01:22<00:00, 682.31audios/s, loss=1.17673, acc=51.00%] \n",
      "100%|██████████| 7488/7488 [00:04<00:00, 1732.33audios/s, loss=0.25892, acc=95.00%]\n",
      "  0%|          | 0/56256 [00:00<?, ?audios/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "total time elapsed: 1h 44m 54s , best accuracy: 95.00%, best loss 0.224434\n",
      "epoch  63 with lr=1.00e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56256/56256 [01:22<00:00, 680.41audios/s, loss=1.17576, acc=51.00%]\n",
      "100%|██████████| 7488/7488 [00:04<00:00, 1816.91audios/s, loss=0.24432, acc=95.00%]\n",
      "  0%|          | 0/56256 [00:00<?, ?audios/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "total time elapsed: 1h 46m 21s , best accuracy: 95.00%, best loss 0.224434\n",
      "epoch  64 with lr=1.00e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56256/56256 [01:22<00:00, 680.11audios/s, loss=1.17981, acc=51.00%] \n",
      "100%|██████████| 7488/7488 [00:04<00:00, 1744.37audios/s, loss=0.24525, acc=95.00%]\n",
      "  0%|          | 0/56256 [00:00<?, ?audios/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "total time elapsed: 1h 47m 48s , best accuracy: 95.00%, best loss 0.224434\n",
      "epoch  65 with lr=1.00e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56256/56256 [01:22<00:00, 678.25audios/s, loss=1.17803, acc=51.00%]\n",
      "100%|██████████| 7488/7488 [00:04<00:00, 1730.41audios/s, loss=0.25848, acc=95.00%]\n",
      "  0%|          | 0/56256 [00:00<?, ?audios/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "total time elapsed: 1h 49m 15s , best accuracy: 95.00%, best loss 0.224434\n",
      "epoch  66 with lr=1.00e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56256/56256 [01:23<00:00, 676.48audios/s, loss=1.17477, acc=50.00%] \n",
      "100%|██████████| 7488/7488 [00:04<00:00, 1667.75audios/s, loss=0.23957, acc=95.00%]\n",
      "  0%|          | 0/56256 [00:00<?, ?audios/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "total time elapsed: 1h 50m 43s , best accuracy: 95.00%, best loss 0.224434\n",
      "epoch  67 with lr=1.00e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56256/56256 [01:22<00:00, 680.39audios/s, loss=1.17670, acc=50.00%] \n",
      "100%|██████████| 7488/7488 [00:04<00:00, 1694.78audios/s, loss=0.25098, acc=94.00%]\n",
      "  0%|          | 0/56256 [00:00<?, ?audios/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "total time elapsed: 1h 52m 10s , best accuracy: 95.00%, best loss 0.224434\n",
      "epoch  68 with lr=1.00e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56256/56256 [01:23<00:00, 676.65audios/s, loss=1.17717, acc=51.00%] \n",
      "100%|██████████| 7488/7488 [00:04<00:00, 1751.32audios/s, loss=0.24909, acc=95.00%]\n",
      "  0%|          | 0/56256 [00:00<?, ?audios/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "total time elapsed: 1h 53m 38s , best accuracy: 95.00%, best loss 0.224434\n",
      "epoch  69 with lr=1.00e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56256/56256 [01:22<00:00, 683.59audios/s, loss=1.17757, acc=51.00%]\n",
      "100%|██████████| 7488/7488 [00:04<00:00, 1664.34audios/s, loss=0.23782, acc=95.00%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "total time elapsed: 1h 55m 5s , best accuracy: 95.00%, best loss 0.224434\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "print(\"training %s for Google speech commands...\" % args.model)\n",
    "since = time.time()\n",
    "for epoch in range(start_epoch, args.max_epochs):\n",
    "    if args.lr_scheduler == 'step':\n",
    "        lr_scheduler.step()\n",
    "\n",
    "    train(epoch)\n",
    "    epoch_loss = valid(epoch)\n",
    "\n",
    "    if args.lr_scheduler == 'plateau':\n",
    "        print(type(epoch_loss))\n",
    "        lr_scheduler.step(metrics=epoch_loss)\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    time_str = 'total time elapsed: {:.0f}h {:.0f}m {:.0f}s '.format(time_elapsed // 3600, time_elapsed % 3600 // 60, time_elapsed % 60)\n",
    "    print(\"%s, best accuracy: %.02f%%, best loss %f\" % (time_str, best_accuracy, best_loss))\n",
    "    with open('./train_res18.log', 'a+') as f:\n",
    "        f.write(\"%s, epoch: %s, best accuracy: %.02f%%, best loss %f\\n\" % (time_str, epoch,best_accuracy, best_loss))\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
